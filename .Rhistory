manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mean(galton$child), size = 3)
g
mean(galton$child)
library(UsingR)
data(diamond)
library(ggplot2)
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SIN $)")
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g
g = g + geom_point(size = 5, colour = "blue", alpha=0.2)
g
library(UsingR)
> data(diamond)
> library(ggplot2)
> g = ggplot(diamond, aes(x = carat, y = price))
> g = g + xlab("Mass (carats)")
> g = g + ylab("Price (SIN $)")
> g = g + geom_point(size = 7, colour = "black", alpha=0.5)
> g
library(UsingR)
data(diamond)
library(ggplot2)
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SIN $)")
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g
g = g + geom_point(size = 5, colour = "blue", alpha=0.2)
g
g = g + geom_smooth(method = "lm", colour = "black")
g
library(UsingR)
data(diamond)
library(ggplot2)
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SIN $)")
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g = g + geom_point(size = 5, colour = "blue", alpha=0.2)
g
library(UsingR)
data(diamond)
library(ggplot2)
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SIN $)")
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g = g + geom_smooth(method = "lm", colour = "black")
g
View(diamond)
e = c(resid(lm(price ~ 1, data = diamond)),
resid(lm(price ~ carat, data = diamond)))
fit = factor(c(rep("Itc", nrow(diamond)),
rep("Itc, slope", nrow(diamond))))
g = ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit))
g = g + geom_dotplot(binaxis = "y", size = 2, stackdir = "center", binwidth = 20)
g = g + xlab("Fitting approach")
g = g + ylab("Residual price")
g
e = c(resid(lm(price ~ 1, data = diamond)),
resid(lm(price ~ carat, data = diamond)))
fit = factor(c(rep("Itc", nrow(diamond)),
rep("Itc, slope", nrow(diamond))))
g = ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit))
g = g + geom_dotplot(binaxis = "y", dotsize = 2, stackdir = "center", binwidth = 20)
g = g + xlab("Fitting approach")
g = g + ylab("Residual price")
g
lm(price ~ 1, data = diamond)
lm(price ~ carat, data = diamond)
y <- diamond$price; x <- diamond$carat
fit <- lm(y ~ x)
sumCoef
sumCoef <- summary(fit)$coefficients
sumCoef
sumCoef[1, 2]
seq(min(x), max(x), length = 100)
seq(min(z), max(z), length = 100)
20*12*3*0.13
library(swirl)
swirl()
swirl()
6
dim(InsectSprays)
head(InsectSprays, n = 15)
sA
summary(InsectSprays[ ,2])
sapply(InsectSprays, class)
fit <- lm(spray ~ count, data = InsectSprays)
fit <- lm(count ~ spray, InsectSprays)
summary(fit)coef
summary(fit)$coef
est <- summary(fit)$coef[ ,1]
mean(sA)
mean(sB)
nfit <- lm(count ~ spray - 1, data = InsectSprays)
summary(nfit)$coef
spray2 <- relevel(InsectSprays$spray, "C")
fit2 <- lm(count ~ spray, ddata = spray2)
fit2 <- lm(count ~ spray, data = spray2)
fit2 <- lm(count ~ spray2, data = InsectSprays)
summary(lm(count ~ spray2, data = InsectSprays))$coef
summary(fit2)$coef
mean(sC)
(fit$coef[3] - fit$coef[2]) / 1.6011
(fit$coef[2]-fit$coef[3])/1.6011
dim(hunger)
13
948
names(hunger)
fit <- lm(Numeric ~ Year, data = hunger)
summary(fit)$coef
lmF <- lm(Numeric[hunger$Sex=="Female"] ~ Year, data = hunger)
lmF <- lm(hunger$Numeric[hunger$Sex=="Female"] ~ hunger$Year[hunger$Sex=="Female"])
lmM <- lm(hunger$Numeric[hunger$Sex=="Male"] ~ hunger$Year[hunger$Sex=="Male"])
lmBoth <- lm(Numeric ~ Year + Sex, data = hunger)
summary(lmBoth)
lmInter <- lm(Numeric ~ Year + Sex + Sex*Year, data = hunger)
summary(lmInter)
fit <- lm(y ~ x, out2)
plot(fit, which=1)
fitno <- lm(y ~ x, out2[-1, ])
plot(fitno, which=1)
coef(fit) - coef(fitno)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
sigma <- 1
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd <- 1
rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
sigma1 <- 1
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
resid(fit)[1] / (sigma1 * sqrt(1-hatvalues(fit)[1]))
head(rstudent(fit))
dy <- predict(fitno, out2) - predict(fit, out2)
sum(dy^2) / 2*sigma^2
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
setwd("D:/Data_science")
library(swirl)
swirl()
library(swirl)
sessionInfo()
getOption("repos")
install.packages("car")
library(swirl)
swirl()
library(car)
?car
library(swirl)
swirl()
library(swirl)
swirl()
install.packages("car", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
swirl()
library(swirl)
swirl()
library(swirl)
swirl()
library(swirl)
swirl()
install.packages("car", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
library(swirl)
swirl()
install.packages("car",dependencies=TRUE)
install.packages("car", dependencies = TRUE)
library(swirl)
swirl()
library(swirl)
swirl()
library(swirl)
swirl()
library(swirl)
swirl()
uninstall_course("Regression Models")
swirl()
library(swirl)
swirl()
library(swirl)
swirl()
install.packages("pbkrtest")
install.packages("pbkrtest",dependencies=TRUE)
library(swirl)
swirl()
swirl()
swirl()
swirl()
swirl()
library(swirl)
swirl()
swirl()
install.packages("installr"); library(installr) #load / install+load installr
updateR() # updating R.
library(swirl)
swirl()
swirl()
install.packages("pbkrtest")
install.packages("pbkrtest", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
install.packages("car", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
library(swirl)
swirl()
library(swirl)
swirl()
install.packages("swirl", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
library(swirl)
swirl()
install.packages("C:/Users/Pavel/Downloads/pbkrtest_0.4-4.zip", repos = NULL, type = "win.binary", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
library(swirl)
swirl()
install.packages("C:/Users/Pavel/Downloads/pbkrtest_0.4-1.tar.gz", repos = NULL, type = "source", lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
library(swirl)
swirl()
install.packages(c("nlme", "pbkrtest"), lib="C:/Users/Pavel/Documents/R/R-3.2.4revised/library")
swirl()
library(swirl)
swirl()
install.packages("C:/Users/Pavel/Downloads/pbkrtest_0.4-3.tar.gz", repos = NULL, type = "source")
library(swirl)
swirl()
?shuttle
View(shuttle)
x <- shuttle
head(shuttle)
library(MASS)
head(shuttle)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
install.packages("caret")
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(caret)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength,g=4)
cutCompressiveStrength
p2 <- qplot(CompressiveStrength,index, data=training,fill=cutCompressiveStrength,
geom=c("boxplot","jitter"))
p2
head(training)
p2 <- qplot(CompressiveStrength, data=training,fill=cutCompressiveStrength, geom=c("boxplot","jitter"))
p2
training <- mutate(training, index=1:nrow(training))
library(dplyr)
training <- mutate(training, index=1:nrow(training))
p2 <- qplot(CompressiveStrength, index, data=training,fill=cutCompressiveStrength, geom=c("boxplot","jitter"))
p2
qplot(index, CompressiveStrength, data=training, color=cut2(training$CompressiveStrength,g=4))
qplot(index, CompressiveStrength, data=training, color=cut2(training$CompressiveStrength,g=breaks))
qplot(index, CompressiveStrength, data=training, color=cut2(training$CompressiveStrength,g=10))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer, breaks=20)
hist(log(training$Superplasticizer+1), breaks=20)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
preObj <- preProcess(training[, IL_col_idx], method=c("center", "scale", "pca"), thresh=0.9)
preObj
names(preObj)
names(IL_col_idx)
IL_col_idx
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
new_training <- training[, c(names(training)[IL_col_idx], "diagnosis")]
IL_col_idx <- grep("^[Ii][Ll].*", names(testing))
suppressMessages(library(dplyr))
new_testing <- testing[, c(names(testing)[IL_col_idx], "diagnosis")]
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
# apply the non pca model on the testing set and check the accuracy
non_pca_result <- confusionMatrix(new_testing[, 13], predict(non_pca_model, new_testing[, -13]))
non_pca_resul
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
install.packages("e1071")
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
non_pca_result <- confusionMatrix(new_testing[, 13], predict(non_pca_model, new_testing[, -13]))
non_pca_result
install.packages("AppliedPredictiveModeling")
install.packages("ElemStatLearn")
install.packages("pgmm")
install.packages("rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
head(segmentationOriginal)
names(segmentationOriginal)
names(segmentationOriginal)
inTrain <- createDataPartition(y=Case$segmentationOriginal,p=0.7, list=FALSE)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~ .,method="rpart",data=training)
print(modFit$finalModel)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=training)
print(modFit$finalModel)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.6, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=training)
print(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.6, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=training)
print(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case,p=0.6, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
print(modFit$finalModel)
suppressMessages(library(rattle))
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
install.packages("rpart.plot")
library(rattle)
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
View(newdata)
newdata
modolive <- train(Area ~ ., method = "rpart", data = olive)
predict(modolive, newdata = newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
missClass = function(values, prediction){sum(((prediction > 0.5) * 1) != values) / length(values)}
modelSA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass(testSA$chd, predict(modelSA, newdata = testSA))
missClass(trainSA$chd, predict(modelSA, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
install.packages("randomForest")
library(randomForest)
modvowel <- randomForest(y ~ ., data = vowel.train)
order(varImp(modvowel), decreasing = T)
setwd("D:/Data_science/Practical mashine learning/Practical-machine-learning")
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
set.seed(12345)
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(trainingURL, na.strings=c("NA",""), header=TRUE)
colNamesTraining <- colnames(training)
testing <- read.csv(testURL, na.strings=c("NA",""), header=TRUE)
colNamesTesting <- colnames(testing)
indexForNA_training <- apply(training,2,function(x) {sum(is.na(x))})
training <- training[,which(indexForNA_training == 0)]
indexForNA_testing <- apply(testing,2,function(x) {sum(is.na(x))})
testing <- testing[,which(indexForNA_testing == 0)]
View(testing)
training <- read.csv(trainingURL, na.strings=c("NA",""), header=TRUE)
testing <- read.csv(testURL, na.strings=c("NA",""), header=TRUE)
View(testing)
View(training)
indexForNA_training <- apply(training,2,function(x) {sum(is.na(x))})
training <- training[,which(indexForNA_training == 0)]
indexForNA_testing <- apply(testing,2,function(x) {sum(is.na(x))})
testing <- testing[,which(indexForNA_testing == 0)]
training$classe <- as.factor(training$classe)
View(training)
View(testing)
set.seed(12345)
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(trainingURL, na.strings=c("NA",""), header=TRUE)
testing <- read.csv(testURL, na.strings=c("NA",""), header=TRUE)
indexForNA_training <- apply(training,2,function(x) {sum(is.na(x))})
training <- training[,which(indexForNA_training == 0)]
indexForNA_testing <- apply(testing,2,function(x) {sum(is.na(x))})
testing <- testing[,which(indexForNA_testing == 0)]
training$classe <- as.factor(training$classe)
training <- read.csv(trainingURL, na.strings=c("NA",""), header=TRUE)
testing <- read.csv(testURL, na.strings=c("NA",""), header=TRUE)
indexForNA_training <- apply(training,2,function(x) {sum(is.na(x))})
training <- training[,which(indexForNA_training == 0)]
indexForNA_testing <- apply(testing,2,function(x) {sum(is.na(x))})
testing <- testing[,which(indexForNA_testing == 0)]
View(training)
training$classe <- as.factor(training$classe)
numericCol <- which(lapply(training, class) %in% "numeric")
preObj <-preProcess(training[,numericCol],method=c('knnImpute', 'center', 'scale'))
trainPreProcessed <- predict(preObj, training[,numericCol])
trainPreProcessed$classe <- training$classe
testPreProcessed <-predict(preObj,testing[,numericCol])
nzvTraining <- nearZeroVar(trainPreProcessed,saveMetrics=TRUE)
trainPreProcessed <- trainPreProcessed[,nzvTraining$nzv==FALSE]
library(RColorBrewer)
testingPreProcessed <-predict(preObj,testing[,numericCol])
nzvTesting <- nearZeroVar(testingPreProcessed,saveMetrics=TRUE)
testingPreProcessed <- testingPreProcessed[,nzvTesting$nzv==FALSE]
inTrain = createDataPartition(trainPreProcessed$classe, p = 3/4, list=FALSE)
trainingPart = trainPreProcessed[inTrain,]
testingPart = testingPreProcessed[-inTrain,]
modFit <- train(classe ~.,
method="rf",
data=trainingPart,
trControl=trainControl(method='cv'),
number=5,
allowParallel=TRUE )
modFit
trainingPartPrediction <- predict(modFit, trainingPart)
confusionMatrix(trainingPartPrediction, trainingPart$classe)
testingPartPrediction <- predict(modFit, testingPart)
confusionMatrix(testingPartPrediction, testingPart$classe)
View(testPreProcessed)
View(testingPart)
testingPart = trainPreProcessed[-inTrain,]
testingPartPrediction <- predict(modFit, testingPart)
confusionMatrix(testingPartPrediction, testingPart$classe)
testingPrediction <- predict(modFit, testingPreProcessed)
testingPrediction
decisiontree <- train(classe~.,method="rpart", data=trainingPart)
fancyRpartPlot(decisiontree$finalModel)
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
decisiontree <- train(classe~.,method="rpart", data=trainingPart)
fancyRpartPlot(decisiontree$finalModel)
